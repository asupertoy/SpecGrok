#!/usr/bin/env python3
"""
æµ‹è¯• IngestionPipelineWrapper çš„è„šæœ¬ã€‚
"""

import os
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))

from llama_index.core import Settings, StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from src.models.embedding import get_embed_model, BgeM3Service
from src.database.qdrant_manager import qdrant_manager
from src.ingestion.indexmannager import IndexManager
from src.ingestion.pipeline import IngestionPipelineWrapper
from src.config import settings

# è®¾ç½®å…¨å±€åµŒå…¥æ¨¡å‹
Settings.embed_model = get_embed_model()

# --- é€‚é…å™¨å¼€å§‹ ---
def custom_sparse_doc_fn(texts: list[str]):
    """ä½¿ç”¨å…¨å±€ BGE-M3 æ¨¡å‹ç”Ÿæˆæ–‡æ¡£ç¨€ç–å‘é‡ (Adapter wrapper)"""
    return BgeM3Service.get_sparse_embedding_adapter(texts)

def custom_sparse_query_fn(text: str):
    """ä½¿ç”¨å…¨å±€ BGE-M3 æ¨¡å‹ç”ŸæˆæŸ¥è¯¢ç¨€ç–å‘é‡"""
    if not text:
        return ([], [])
    indices, values = BgeM3Service.get_sparse_embedding_adapter([text])
    return (indices[0], values[0])
# --- é€‚é…å™¨ç»“æŸ ---

def verify_pipeline_processing(pipeline, index_manager, file_name, content):
    """
    è¾…åŠ©å‡½æ•°ï¼šéªŒè¯å•ä¸ªæ–‡ä»¶çš„å®Œæ•´å¤„ç†æµç¨‹ (Load -> Parse -> Chunk -> Embed -> Upsert)
    """
    file_path = project_root / file_name
    print(f"\n{'='*30}\nğŸ” æ­£åœ¨æµ‹è¯•æ–‡ä»¶: {file_name}\n{'='*30}")
    
    # å†™å…¥æµ‹è¯•æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    try:
        # 1. Load
        print(f"[1] Loading file...")
        blobs = pipeline.loader.load(str(file_path))
        print(f"    - Loaded blobs: {[b.source for b in blobs]}")
        assert len(blobs) > 0, "Loader æœªèƒ½åŠ è½½ä»»ä½• Blob"

        # 2. Parse
        print(f"[2] Parsing content...")
        parsed_nodes = []
        for b in blobs:
            ns = pipeline._parse_blob(b)
            print(f"    - Parsed {len(ns)} section nodes from {b.source}")
            parsed_nodes.extend(ns)
        assert len(parsed_nodes) > 0, "è§£æé˜¶æ®µæœªäº§å‡ºä»»ä½•èŠ‚ç‚¹"

        # 3. Chunking
        print(f"[3] Chunking nodes...")
        chunked_nodes = pipeline.chunking.run_chunking(parsed_nodes)
        print(f"    - Generated {len(chunked_nodes)} chunks")
        assert len(chunked_nodes) > 0, "Chunking æœªäº§å‡ºä»»ä½• chunk"
        
        # éªŒè¯ Chunk è¯¦æƒ…
        for i, n in enumerate(chunked_nodes[:3]):
            preview = n.text[:50].replace('\n', ' ')
            print(f"    - Chunk[{i}] (ref_doc_id={n.ref_doc_id}): {preview}...")
            # éªŒè¯å¼•ç”¨IDç»§æ‰¿
            assert n.ref_doc_id is not None, f"Chunk[{i}] ä¸¢å¤±äº† ref_doc_id"
            if settings.ENABLE_HYBRID and 'sparse_values' in n.metadata:
                assert isinstance(n.metadata['sparse_values'], dict), "Sparse values æ ¼å¼é”™è¯¯"

        # 4. Embedding
        print(f"[4] Generating embeddings...")
        index_manager._ensure_embeddings(chunked_nodes)
        post_embedded = sum(1 for n in chunked_nodes if n.embedding is not None)
        print(f"    - Embeddings generated for {post_embedded}/{len(chunked_nodes)} nodes")
        assert post_embedded == len(chunked_nodes), "éƒ¨åˆ† Chunk ç¼ºå¤± Embedding"

        # 5. Upsert
        print(f"[5] Upserting to Vector Store...")
        
        # è·å– upsert å‰çš„æ•°é‡
        client = qdrant_manager.get_client()
        try:
            if client.collection_exists(settings.QDRANT_COLLECTION_NAME):
                before_count = client.get_collection(settings.QDRANT_COLLECTION_NAME).points_count
            else:
                before_count = 0
        except Exception:
            before_count = 0

        index_manager.upsert_nodes(chunked_nodes)
        
        # ç­‰å¾…å¼‚æ­¥å†™å…¥
        time.sleep(1.0)

        # è·å– upsert åçš„æ•°é‡
        try:
            after_count = client.get_collection(settings.QDRANT_COLLECTION_NAME).points_count
        except Exception:
            after_count = -1
            
        print(f"    - Qdrant points count: {before_count} -> {after_count}")
        assert after_count >= before_count + len(chunked_nodes) or after_count > 0, "Upsert åæ•°æ®é‡æœªæ­£å¸¸å¢åŠ "

        print(f"âœ… æ–‡ä»¶ {file_name} æµ‹è¯•é€šè¿‡")

    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if file_path.exists():
            file_path.unlink()
            print(f"ğŸ§¹ æ¸…ç†æ–‡ä»¶: {file_name}")

def test_pipeline():
    """æµ‹è¯•å®Œæ•´çš„ ingestion pipelineï¼Œè¦†ç›– TXT, MD, HTMLã€‚"""
    print("ğŸš€ å¼€å§‹ Pipeline é›†æˆæµ‹è¯• (TXT, MD, HTML)...")

    # 1. åˆå§‹åŒ–ç»„ä»¶
    qdrant_client = qdrant_manager.get_client()
    
    # æ¸…ç†ç¯å¢ƒï¼šä¸ºäº†æµ‹è¯•å‡†ç¡®æ€§ï¼Œæ¯æ¬¡è¿è¡Œå‰æ¸…ç†é›†åˆ
    if qdrant_client.collection_exists(settings.QDRANT_COLLECTION_NAME):
        print(f"âš ï¸  æ¸…ç†æ—§é›†åˆ: {settings.QDRANT_COLLECTION_NAME}")
        qdrant_client.delete_collection(settings.QDRANT_COLLECTION_NAME)

    # åˆå§‹åŒ– Vector Store
    vector_store_kwargs = {
        "client": qdrant_client,
        "collection_name": settings.QDRANT_COLLECTION_NAME,
        "enable_hybrid": settings.ENABLE_HYBRID,
    }
    if settings.ENABLE_HYBRID:
        print("ğŸ”§ å¯ç”¨ Hybrid Search Adapter")
        vector_store_kwargs["sparse_doc_fn"] = custom_sparse_doc_fn
        vector_store_kwargs["sparse_query_fn"] = custom_sparse_query_fn

    vector_store = QdrantVectorStore(**vector_store_kwargs)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index_manager = IndexManager(storage_context=storage_context)
    pipeline = IngestionPipelineWrapper(index_manager=index_manager)

    # 2. å®šä¹‰æµ‹è¯•ç”¨ä¾‹ (æ–‡ä»¶å, å†…å®¹)
    test_cases = [
        ("test_sample.txt", 
            r"""
                è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„æµ‹è¯•æ–‡æ¡£ã€‚
                ### ç¬¬ä¸€éƒ¨åˆ†
                - é¡¹ç›®1
                - å­é¡¹ç›®1.1
                - å­é¡¹ç›®1.2
                - é¡¹ç›®2

                #### ç¬¬äºŒéƒ¨åˆ†
                æ•°å­¦å…¬å¼ï¼š$$ \int_{0}^{\infty} e^{-x} dx = 1 $$

                ä»£ç ç¤ºä¾‹ï¼š
                    def hello():
                        print("Hello World")
                        if True:
                            return 42
            """),
                    
        ("test_sample.md", 
            r"""
                # å¤æ‚Markdownæ–‡æ¡£

                ## ä»‹ç»
                è¿™æ˜¯ä¸€ä¸ªåµŒå¥—ç»“æ„çš„æ–‡æ¡£ã€‚

                ### åˆ—è¡¨éƒ¨åˆ†
                - é¡¶çº§é¡¹ç›®
                - å­é¡¹ç›®A
                    - æ·±å±‚å­é¡¹ç›®A1
                - å­é¡¹ç›®B
                - å¦ä¸€ä¸ªé¡¶çº§é¡¹ç›®

                ## ä»£ç å’Œå…¬å¼
                ```python
                def complex_function():
                    if condition:
                        for i in range(10):
                            print(f"Item {i}")
                    return result
                ```

                å†…è”å…¬å¼ï¼š$ E = mc^2 $ å’Œå—å…¬å¼ï¼š
                $$ \sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6} $$
            """),

        ("test_sample.html", 
            r"""
                <!DOCTYPE html>
                <html>
                <body>
                    <h1>å¤æ‚HTMLæ–‡æ¡£</h1>
                    <div>
                        <h2>åµŒå¥—ç»“æ„</h2>
                        <p>æ®µè½å†…å®¹ã€‚</p>
                        <ul>
                            <li>é¡¹ç›®1
                                <ul>
                                    <li>å­é¡¹ç›®1.1</li>
                                    <li>å­é¡¹ç›®1.2</li>
                                </ul>
                            </li>
                            <li>é¡¹ç›®2</li>
                        </ul>
                        <h3>ä»£ç éƒ¨åˆ†</h3>
                        <pre><code>
                def example():
                    if True:
                        print("Indented code")
                        for x in list:
                            process(x)
                        </code></pre>
                        <p>æ•°å­¦å…¬å¼ï¼š$$ a^2 + b^2 = c^2 $$</p>
                    </div>
                </body>
                </html>
            """)
    ]

    # 3. å¾ªç¯æ‰§è¡Œæµ‹è¯•
    try:
        for fname, content in test_cases:
            verify_pipeline_processing(pipeline, index_manager, fname, content)

        # 4. æµ‹è¯• pipeline.run() æ•´ä½“æµç¨‹åŠç»Ÿè®¡
        print(f"\n{'='*30}\nğŸ“Š Testing pipeline.run() & Stats\n{'='*30}")
        run_file = project_root / "test_run_stats.txt"
        with open(run_file, 'w', encoding='utf-8') as f:
            f.write("Stats test content.")
            
        try:
            pipeline.run(str(run_file))
            stats = pipeline.get_stats()
            print("Pipeline Stats:", stats)
            assert stats["processed_files"] >= 1
            assert stats["generated_chunks"] >= 1
        finally:
            if run_file.exists():
                run_file.unlink()

        # æœ€ç»ˆ Index çŠ¶æ€
        idx_stats = index_manager.get_index_stats()
        print("\nğŸ“ˆ Final Index Stats:", idx_stats)
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    test_pipeline()
