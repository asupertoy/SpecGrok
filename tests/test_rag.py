#!/usr/bin/env python3
"""
æµ‹è¯•å®Œæ•´çš„ RAG ç³»ç»Ÿï¼šæ£€ç´¢ + é‡æ’åº + é—®ç­”
"""

import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root / "src"))

from llama_index.core import Settings, StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.llms.mock import MockLLM
from database.qdrant_manager import qdrant_manager
from ingestion.indexmannager import IndexManager
from engine import create_query_engine
from models.llm import get_llm
from config import settings

# è®¾ç½®å…¨å±€åµŒå…¥æ¨¡å‹ - ä½¿ç”¨æ ‡å‡†çš„ HuggingFace embedding
Settings.embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5",
    trust_remote_code=True,
)
# ä½¿ç”¨ MockLLM é¿å…å¤–éƒ¨ API ä¾èµ–ä¸æ¨¡å‹åæ ¡éªŒ
Settings.llm = MockLLM(max_tokens=256)


def test_rag_system():
    """æµ‹è¯•å®Œæ•´çš„ RAG ç³»ç»Ÿ"""
    print("ğŸš€ å¼€å§‹ RAG ç³»ç»Ÿé›†æˆæµ‹è¯•...")

    # 1. åˆå§‹åŒ–å‘é‡å­˜å‚¨
    qdrant_client = qdrant_manager.get_client()

    # æ¸…ç†æ—§é›†åˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if qdrant_client.collection_exists(settings.QDRANT_COLLECTION_NAME):
        print(f"âš ï¸  æ¸…ç†æ—§é›†åˆ: {settings.QDRANT_COLLECTION_NAME}")
        qdrant_client.delete_collection(settings.QDRANT_COLLECTION_NAME)

    # åˆå§‹åŒ– Vector Store
    vector_store_kwargs = {
        "client": qdrant_client,
        "collection_name": settings.QDRANT_COLLECTION_NAME,
        "enable_hybrid": True,
    }

    # Attach our sparse adapters from BGE so Qdrant hybrid uses the model's lexical outputs
    from models.embedding import BgeM3Service
    vector_store_kwargs["sparse_doc_fn"] = BgeM3Service.sparse_doc_fn
    vector_store_kwargs["sparse_query_fn"] = BgeM3Service.sparse_query_fn

    vector_store = QdrantVectorStore(**vector_store_kwargs)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index_manager = IndexManager(storage_context=storage_context)

    # 2. åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        project_root / "test_rag_doc1.txt",
        project_root / "test_rag_doc2.txt",
    ]

    # æ–‡æ¡£å†…å®¹
    doc_contents = [
        """
        Python ç¼–ç¨‹è¯­è¨€ç®€ä»‹

        Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´åˆ›å»ºã€‚
        Python çš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç çš„å¯è¯»æ€§å’Œç®€æ´æ€§ï¼Œä½¿ç”¨ç¼©è¿›ä»£æ›¿èŠ±æ‹¬å·ã€‚

        ä¸»è¦ç‰¹ç‚¹ï¼š
        1. ç®€å•æ˜“å­¦ï¼šè¯­æ³•ç®€æ´æ˜äº†
        2. åŠŸèƒ½å¼ºå¤§ï¼šæ”¯æŒé¢å‘å¯¹è±¡ã€å‡½æ•°å¼ç¼–ç¨‹
        3. ç”Ÿæ€ä¸°å¯Œï¼šæ‹¥æœ‰å¤§é‡çš„ç¬¬ä¸‰æ–¹åº“
        4. è·¨å¹³å°ï¼šå¯ä»¥åœ¨ Windowsã€Linuxã€macOS ä¸Šè¿è¡Œ

        Python åœ¨æ•°æ®ç§‘å­¦ã€Web å¼€å‘ã€è‡ªåŠ¨åŒ–è„šæœ¬ç­‰é¢†åŸŸåº”ç”¨å¹¿æ³›ã€‚
        """,
        """
        æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ

        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚

        ç›‘ç£å­¦ä¹ ï¼š
        - çº¿æ€§å›å½’ï¼šé¢„æµ‹è¿ç»­å€¼
        - é€»è¾‘å›å½’ï¼šåˆ†ç±»é—®é¢˜
        - å†³ç­–æ ‘ï¼šåŸºäºç‰¹å¾çš„åˆ†ç±»

        æ— ç›‘ç£å­¦ä¹ ï¼š
        - K-means èšç±»ï¼šæ•°æ®åˆ†ç»„
        - ä¸»æˆåˆ†åˆ†æï¼šé™ç»´
        - å…³è”è§„åˆ™æŒ–æ˜ï¼šå‘ç°æ•°æ®å…³è”

        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚
        """
    ]

    # å†™å…¥æµ‹è¯•æ–‡ä»¶
    for file_path, content in zip(test_docs, doc_contents):
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content.strip())

    try:
        # 3. ç´¢å¼•æ–‡æ¡£
        print("\nğŸ“š ç´¢å¼•æµ‹è¯•æ–‡æ¡£...")
        from ingestion.pipeline import IngestionPipelineWrapper
        pipeline = IngestionPipelineWrapper(index_manager=index_manager)

        for doc_file in test_docs:
            print(f"  ç´¢å¼•æ–‡æ¡£: {doc_file.name}")
            pipeline.run(str(doc_file))

        # 4. åˆ›å»ºæŸ¥è¯¢å¼•æ“
        print("\nğŸ” åˆ›å»ºæŸ¥è¯¢å¼•æ“...")
        # è·å–ç´¢å¼•ï¼ˆå‡è®¾åªæœ‰ä¸€ä¸ªç´¢å¼•ï¼‰
        index = index_manager.index
        if index is None:
            # å¦‚æœæ²¡æœ‰ç´¢å¼•ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ç„¶åæ·»åŠ èŠ‚ç‚¹
            from llama_index.core import VectorStoreIndex
            index = VectorStoreIndex.from_vector_store(vector_store)

        # Disable external API reranker in tests to avoid external model issues
        query_engine = create_query_engine(
            index=index,
            similarity_top_k=10,
            rerank_top_n=5,
            alpha=0.5,
            use_reranker=False,
        )

        # 5. æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "Python çš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "ä»€ä¹ˆæ˜¯ç›‘ç£å­¦ä¹ ï¼Ÿ",
            "Python åœ¨å“ªäº›é¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Ÿ",
            "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
        ]

        print("\nâ“ æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢...")
        for i, query in enumerate(test_queries, 1):
            print(f"\n--- æŸ¥è¯¢ {i}: {query} ---")

            # è·å–å¸¦æ¥æºçš„å›ç­”
            result = query_engine.query_with_sources(query)

            print(f"å›ç­”: {result['answer'][:200]}...")
            print(f"æ¥æºæ–‡æ¡£æ•°é‡: {len(result['sources'])}")

            if result['sources']:
                print("Top æ¥æº:")
                for source in result['sources'][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
                    print(f"  - æ–‡æ¡£ {source['index']}: è¯„åˆ† {source.get('score', 'N/A'):.3f}")
                    print(f"    å†…å®¹é¢„è§ˆ: {source['content'][:100]}...")

        # 6. éªŒè¯ç³»ç»Ÿç»Ÿè®¡
        print("\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡...")
        index_stats = index_manager.get_index_stats()
        print(f"ç´¢å¼•ç»Ÿè®¡: {index_stats}")

        print("\nğŸ‰ RAG ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼æ‰€æœ‰æŸ¥è¯¢éƒ½æˆåŠŸå¤„ç†ã€‚")

    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        for doc_file in test_docs:
            if doc_file.exists():
                doc_file.unlink()
                print(f"ğŸ§¹ æ¸…ç†æ–‡ä»¶: {doc_file.name}")


if __name__ == "__main__":
    test_rag_system()