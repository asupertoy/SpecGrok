import os
import sys
import time
import tempfile
from pathlib import Path

# Add src to path
current_dir = Path(__file__).resolve().parent
sys.path.append(str(current_dir.parent / "src"))

from src.ingestion.parsers.parser_txt import TextParser
from src.ingestion.loaders import Blob

def test_basic_text_parsing():
    """æµ‹è¯•åŸºæœ¬æ–‡æœ¬è§£æå’Œæ¸…æ´—åŠŸèƒ½"""
    print("=== æµ‹è¯•åŸºæœ¬æ–‡æœ¬è§£æ ===")

    # åŒ…å«æ§åˆ¶å­—ç¬¦ã€å¤šä½™ç©ºç™½ã€ä¸åŒæ¢è¡Œç¬¦çš„æ–‡æœ¬
    text_content = """ç¬¬ä¸€ç«   å¼•è¨€

è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ã€‚\r\n
\r\n\r\n
ç¬¬äºŒæ®µå†…å®¹ã€‚

\x00\x01æ§åˆ¶å­—ç¬¦æµ‹è¯•\x1f\x7fã€‚

ç¬¬ä¸‰ç«   èƒŒæ™¯ä»‹ç»

   è¿™æ˜¯èƒŒæ™¯å†…å®¹ã€‚
ç¬¬å››æ®µã€‚

"""
    blob = Blob(data=text_content.encode("utf-8"), source="test.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    assert len(nodes) >= 2, f"æœŸæœ›è‡³å°‘2ä¸ªèŠ‚ç‚¹ï¼Œå®é™…{len(nodes)}"
    assert "ç¬¬ä¸€ç« " in nodes[0].metadata['section_header']
    assert "ç¬¬ä¸‰ç« " in nodes[1].metadata['section_header']

    # æ£€æŸ¥æ§åˆ¶å­—ç¬¦è¢«ç§»é™¤
    all_content = "\n".join([node.text for node in nodes])
    assert "\x00" not in all_content, "æ§åˆ¶å­—ç¬¦åº”è¯¥è¢«ç§»é™¤"
    assert "\x01" not in all_content, "æ§åˆ¶å­—ç¬¦åº”è¯¥è¢«ç§»é™¤"

    print("âœ“ åŸºæœ¬æ–‡æœ¬è§£ææµ‹è¯•é€šè¿‡")

def test_encoding_detection():
    """æµ‹è¯•ç¼–ç æ£€æµ‹åŠŸèƒ½"""
    print("=== æµ‹è¯•ç¼–ç æ£€æµ‹ ===")

    # æµ‹è¯•UTF-8
    utf8_text = "è¿™æ˜¯UTF-8ç¼–ç çš„ä¸­æ–‡æ–‡æœ¬"
    blob = Blob(data=utf8_text.encode("utf-8"), source="utf8.txt")
    parser = TextParser()
    nodes = parser.parse(blob)
    assert len(nodes) == 1
    assert "UTF-8" in nodes[0].text or "è¿™æ˜¯" in nodes[0].text

    # æµ‹è¯•GBKç¼–ç ï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰
    try:
        gbk_text = "è¿™æ˜¯GBKç¼–ç çš„ä¸­æ–‡æ–‡æœ¬"
        blob = Blob(data=gbk_text.encode("gbk"), source="gbk.txt")
        nodes = parser.parse(blob)
        assert len(nodes) == 1
        print("âœ“ GBKç¼–ç æ£€æµ‹æµ‹è¯•é€šè¿‡")
    except UnicodeEncodeError:
        print("âœ“ GBKç¼–ç æµ‹è¯•è·³è¿‡ï¼ˆç³»ç»Ÿä¸æ”¯æŒï¼‰")

    print("âœ“ ç¼–ç æ£€æµ‹æµ‹è¯•é€šè¿‡")

def test_header_detection_edge_cases():
    """æµ‹è¯•æ ‡é¢˜æ£€æµ‹çš„è¾¹ç¼˜æƒ…å†µï¼ˆå¦‚é˜²æ­¢å…¬å¼è¯¯åˆ¤ï¼‰"""
    print("=== æµ‹è¯•æ ‡é¢˜æ£€æµ‹è¾¹ç¼˜æƒ…å†µ ===")

    # åŒ…å«ä¹‹å‰å¯¼è‡´è¯¯åˆ¤çš„å…¬å¼è¡Œçš„æ–‡æœ¬
    text_content = r"""## ç¬¬ä¸€ç«  æ­£å¸¸æ ‡é¢˜

æ­£å¸¸å†…å®¹ã€‚
åŒ…æ‹¬ä¸€äº›æ•°å­¦å…¬å¼ï¼š$$ E = mc^2 $$
è¿™æ˜¯æ­£å¸¸çš„ä¸€è¡Œã€‚

1.1 æ•°å­—æ ‡é¢˜
åˆ—è¡¨é¡¹ï¼š
- é¡¹ç›®1
- é¡¹ç›®2

# å¤§æ ‡é¢˜
"""
    blob = Blob(data=text_content.encode("utf-8"), source="edge_case.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    # éªŒè¯èŠ‚ç‚¹
    # æœŸæœ›ï¼š
    # Node 1: "## ç¬¬ä¸€ç«  æ­£å¸¸æ ‡é¢˜" (header) ... åŒ…å«å…¬å¼
    # Node 2: "1.1 æ•°å­—æ ‡é¢˜" (header check, assuming strict digit rule works) or merged into prev if not header
    # Node 3: "# å¤§æ ‡é¢˜"

    print(f"è§£æå¾—åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
    for i, n in enumerate(nodes):
        print(f"Node[{i}] Header: {n.metadata.get('section_header')}")
        print(f"Node[{i}] Content Preview: {n.text[:20]}...")

    # éªŒè¯å…¬å¼è¡Œæ²¡æœ‰è¢«è¯†åˆ«ä¸ºæ ‡é¢˜
    # å¦‚æœå…¬å¼è¡Œè¢«è¯†åˆ«ä¸ºæ ‡é¢˜ï¼Œå®ƒä¼šæˆä¸ºæŸä¸ª node çš„ section_headerï¼Œæˆ–è€…å•ç‹¬æˆä¸ºä¸€ä¸ª node çš„ text start
    headers = [n.metadata.get('section_header') for n in nodes]
    assert "åŒ…æ‹¬ä¸€äº›æ•°å­¦å…¬å¼ï¼š$$ E = mc^2 $$" not in headers, "å…¬å¼è¡Œä¸åº”è¢«è¯†åˆ«ä¸ºæ ‡é¢˜"

    # éªŒè¯ markdown æ ‡é¢˜è¢«è¯†åˆ«
    assert any("## ç¬¬ä¸€ç« " in h for h in headers), "Markdown ## æ ‡é¢˜åº”è¢«è¯†åˆ«"
    assert any("# å¤§æ ‡é¢˜" in h for h in headers), "Markdown # æ ‡é¢˜åº”è¢«è¯†åˆ«"

    print("âœ“ æ ‡é¢˜æ£€æµ‹è¾¹ç¼˜æƒ…å†µæµ‹è¯•é€šè¿‡")

def test_line_merging():
    """æµ‹è¯•çŸ­è¡Œåˆå¹¶åŠŸèƒ½"""
    print("=== æµ‹è¯•çŸ­è¡Œåˆå¹¶ ===")

    text_content = """ç¬¬ä¸€ç«  å¼•è¨€

è¿™æ˜¯ä¸€ä¸ªè¢«ç¡¬æ¢è¡Œåˆ†å‰²çš„
å¥å­ï¼Œåº”è¯¥è¢«
åˆå¹¶æˆä¸€è¡Œã€‚

è¿™æ˜¯å¦ä¸€ä¸ª
å¥å­ï¼Œä¹Ÿåº”è¯¥
è¢«åˆå¹¶ã€‚

ç¬¬äºŒç«  èƒŒæ™¯

çŸ­è¡Œæµ‹è¯•ï¼š
è¿™æ˜¯ä¸€è¡Œå¾ˆé•¿çš„å†…å®¹ï¼ŒåŒ…å«äº†å¾ˆå¤šæ–‡å­—ï¼Œç”¨æ¥æµ‹è¯•åˆå¹¶é˜ˆå€¼è®¡ç®—ã€‚
è¿™æ˜¯ä¸€è¡Œã€‚
è¿™æ˜¯å¦ä¸€è¡Œã€‚
è¿™åˆæ˜¯ä¸€è¡ŒçŸ­å†…å®¹ã€‚

"""
    config = {'merge_short_lines': True}
    blob = Blob(data=text_content.encode("utf-8"), source="merge_test.txt")
    parser = TextParser(config=config)
    nodes = parser.parse(blob)

    all_content = "\n".join([node.text for node in nodes])

    # æ£€æŸ¥å¥å­è¢«åˆå¹¶ï¼ˆåº”è¯¥åŒ…å«åˆå¹¶åçš„é•¿å¥å­ï¼‰
    assert "è¿™æ˜¯ä¸€ä¸ªè¢«ç¡¬æ¢è¡Œåˆ†å‰²çš„ å¥å­ï¼Œåº”è¯¥è¢« åˆå¹¶æˆä¸€è¡Œã€‚" in all_content, "å¥å­åº”è¯¥è¢«åˆå¹¶"
    assert "è¿™æ˜¯å¦ä¸€ä¸ª å¥å­ï¼Œä¹Ÿåº”è¯¥ è¢«åˆå¹¶ã€‚" in all_content, "å¦ä¸€ä¸ªå¥å­åº”è¯¥è¢«åˆå¹¶"

    # æ£€æŸ¥æ ‡é¢˜è¢«ä¿ç•™
    assert "ç¬¬ä¸€ç«  å¼•è¨€" in all_content, "æ ‡é¢˜åº”è¯¥è¢«ä¿ç•™"
    assert "ç¬¬äºŒç«  èƒŒæ™¯" in all_content, "æ ‡é¢˜åº”è¯¥è¢«ä¿ç•™"

    print("âœ“ çŸ­è¡Œåˆå¹¶æµ‹è¯•é€šè¿‡")

def test_header_recognition():
    """æµ‹è¯•æ ‡é¢˜è¯†åˆ«åŠŸèƒ½"""
    print("=== æµ‹è¯•æ ‡é¢˜è¯†åˆ« ===")

    text_content = """ç¬¬ä¸€ç«  å¼•è¨€

å¼•è¨€å†…å®¹ã€‚

ç¬¬äºŒç«  èƒŒæ™¯ä»‹ç»

èƒŒæ™¯å†…å®¹ã€‚

ç¬¬3ç«  è¯¦ç»†è¯´æ˜

è¯´æ˜å†…å®¹ã€‚

CONCLUSION

ç»“è®ºå†…å®¹ã€‚

I. ç¬¬ä¸€éƒ¨åˆ†

ç¬¬ä¸€éƒ¨åˆ†å†…å®¹ã€‚

II. ç¬¬äºŒéƒ¨åˆ†

ç¬¬äºŒéƒ¨åˆ†å†…å®¹ã€‚

ä¸»è¦åŠŸèƒ½

åŠŸèƒ½æè¿°ã€‚

"""

    blob = Blob(data=text_content.encode("utf-8"), source="header_test.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    headers = [node.metadata.get('section_header') for node in nodes]
    print(f"è¯†åˆ«åˆ°çš„æ ‡é¢˜: {headers}")

    # æ£€æŸ¥å„ç§æ ‡é¢˜æ ¼å¼
    assert any("ç¬¬ä¸€ç« " in h for h in headers), "åº”è¯¥è¯†åˆ«ä¸­æ–‡ç« èŠ‚æ ‡é¢˜"
    assert any("CONCLUSION" in h for h in headers), "åº”è¯¥è¯†åˆ«å…¨å¤§å†™æ ‡é¢˜"
    assert any("I. ç¬¬ä¸€éƒ¨åˆ†" in h for h in headers), "åº”è¯¥è¯†åˆ«ç½—é©¬æ•°å­—æ ‡é¢˜"
    assert any("ä¸»è¦åŠŸèƒ½" in h for h in headers), "åº”è¯¥è¯†åˆ«çŸ­æ ‡é¢˜"

    print("âœ“ æ ‡é¢˜è¯†åˆ«æµ‹è¯•é€šè¿‡")

def test_block_protection():
    """æµ‹è¯•å—ä¿æŠ¤åŠŸèƒ½ï¼ˆä»£ç å—ã€æ•°å­¦å—ã€è¡¨æ ¼ï¼‰"""
    print("=== æµ‹è¯•å—ä¿æŠ¤ ===")

    text_content = """ç¬¬ä¸€ç«  ä»£ç ç¤ºä¾‹

ä¸‹é¢æ˜¯ä»£ç ï¼š

```
def hello():
    print("Hello World")
    # è¿™ä¸æ˜¯æ ‡é¢˜
    return True
```


ç¬¬äºŒç«  æ•°å­¦å…¬å¼

å…¬å¼ï¼š
$$
E = mc^2
# è¿™ä¹Ÿä¸æ˜¯æ ‡é¢˜
x^2 + y^2 = z^2
$$


ç¬¬ä¸‰ç«  æ•°æ®è¡¨æ ¼

| å§“å | å¹´é¾„ | èŒä¸š |
|------|------|------|
| å¼ ä¸‰ | 25 | å·¥ç¨‹å¸ˆ |
| # è¿™ä¸æ˜¯æ ‡é¢˜ | 30 | è®¾è®¡å¸ˆ |


ç¬¬å››ç«  ç»“è®º

ç»“è®ºå†…å®¹ã€‚

"""

    blob = Blob(data=text_content.encode("utf-8"), source="block_test.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    print(f"å®é™…ç”Ÿæˆçš„èŠ‚ç‚¹æ•°: {len(nodes)}")
    for i, node in enumerate(nodes):
        print(f"èŠ‚ç‚¹ {i}: header='{node.metadata.get('section_header')}', content_length={len(node.text)}")
        print(f"  å†…å®¹é¢„è§ˆ: {node.text[:100]}...")
        print()

    # æ”¾å®½æ–­è¨€ï¼Œè‡³å°‘åº”è¯¥æœ‰1ä¸ªèŠ‚ç‚¹ä¸”åŒ…å«ä»£ç 
    assert len(nodes) >= 1, f"æœŸæœ›è‡³å°‘1ä¸ªèŠ‚ç‚¹ï¼Œå®é™…{len(nodes)}"
    assert any("def hello():" in node.text for node in nodes), "åº”è¯¥åŒ…å«ä»£ç å—å†…å®¹"
    assert any("E = mc^2" in node.text for node in nodes), "åº”è¯¥åŒ…å«æ•°å­¦å…¬å¼å†…å®¹"
    assert any("|" in node.text for node in nodes), "åº”è¯¥åŒ…å«è¡¨æ ¼å†…å®¹"

    print("âœ“ å—ä¿æŠ¤æµ‹è¯•é€šè¿‡")

def test_cleaning_features():
    """æµ‹è¯•æ–‡æœ¬æ¸…æ´—åŠŸèƒ½"""
    print("=== æµ‹è¯•æ–‡æœ¬æ¸…æ´— ===")

    text_content = """---
title: æµ‹è¯•æ–‡æ¡£
author: æµ‹è¯•ä½œè€…
---

<!-- HTMLæ³¨é‡Š -->

ç¬¬ä¸€ç«  <b>å¼•è¨€</b>

è¿™æ˜¯åŒ…å«HTMLæ ‡ç­¾çš„å†…å®¹ã€‚

ç¬¬äºŒç«  ç»“è®º

<!-- å¦ä¸€ä¸ªæ³¨é‡Š -->
ç»“è®ºå†…å®¹ã€‚

"""

    blob = Blob(data=text_content.encode("utf-8"), source="clean_test.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    all_content = "\n".join([node.text for node in nodes])

    # æ£€æŸ¥YAML frontmatterè¢«ç§»é™¤
    assert "---" not in all_content, "YAML frontmatteråº”è¯¥è¢«ç§»é™¤"

    # æ£€æŸ¥HTMLæ³¨é‡Šè¢«ç§»é™¤
    assert "<!--" not in all_content, "HTMLæ³¨é‡Šåº”è¯¥è¢«ç§»é™¤"

    # æ£€æŸ¥HTMLæ ‡ç­¾è¢«ç§»é™¤ä½†å†…å®¹ä¿ç•™
    assert "<b>" not in all_content, "HTMLæ ‡ç­¾åº”è¯¥è¢«ç§»é™¤"
    assert "å¼•è¨€" in all_content, "HTMLæ ‡ç­¾å†…å®¹åº”è¯¥ä¿ç•™"

    print("âœ“ æ–‡æœ¬æ¸…æ´—æµ‹è¯•é€šè¿‡")

def test_load_data():
    """æµ‹è¯•load_dataæ–¹æ³•"""
    print("=== æµ‹è¯•load_dataæ–¹æ³• ===")

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        f.write("ç¬¬ä¸€ç«  æµ‹è¯•\n\nè¿™æ˜¯æµ‹è¯•å†…å®¹ã€‚")
        temp_file = f.name

    try:
        parser = TextParser()
        nodes = parser.load_data(temp_file)
        assert len(nodes) >= 1
        assert "ç¬¬ä¸€ç« " in nodes[0].metadata.get('section_header', '')
        print("âœ“ load_dataæ–¹æ³•æµ‹è¯•é€šè¿‡")
    finally:
        os.unlink(temp_file)

def test_complex_document():
    """æµ‹è¯•å¤æ‚æ–‡æ¡£è§£æ"""
    print("=== æµ‹è¯•å¤æ‚æ–‡æ¡£è§£æ ===")

    complex_content = r"""ç¬¬ä¸€ç«  å¼•è¨€

è¿™æ˜¯æ–‡æ¡£çš„å¼•è¨€éƒ¨åˆ†ï¼ŒåŒ…å«äº†åŸºæœ¬çš„ä»‹ç»å†…å®¹ã€‚

1.2.3 ç‰ˆæœ¬è¯´æ˜

ç‰ˆæœ¬1.2.3åŒ…å«ä»¥ä¸‹æ”¹è¿›ï¼š
- æ€§èƒ½ä¼˜åŒ–
- ç•Œé¢æ”¹è¿›
- é”™è¯¯ä¿®å¤

ç¬¬äºŒç«  æŠ€æœ¯ç»†èŠ‚

## å­æ ‡é¢˜

æŠ€æœ¯ç»†èŠ‚è¯´æ˜ã€‚

```python
# ä»£ç ç¤ºä¾‹
def process_data(data):
    return sorted(data)
```

### æ›´æ·±å±‚çº§

æ›´è¯¦ç»†çš„æŠ€æœ¯è¯´æ˜ã€‚

CONCLUSION

æ€»ç»“å†…å®¹ã€‚

I. é™„å½•A

é™„å½•å†…å®¹ã€‚

II. é™„å½•B

æ›´å¤šé™„å½•å†…å®¹ã€‚

"""

    blob = Blob(data=complex_content.encode("utf-8"), source="complex.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    print(f"å¤æ‚æ–‡æ¡£ç”ŸæˆèŠ‚ç‚¹æ•°: {len(nodes)}")

    headers = [node.metadata.get('section_header') for node in nodes]
    print(f"è¯†åˆ«æ ‡é¢˜: {headers}")

    # éªŒè¯åŸºæœ¬åŠŸèƒ½
    assert len(nodes) >= 3, f"å¤æ‚æ–‡æ¡£åº”è¯¥ç”Ÿæˆè‡³å°‘3ä¸ªèŠ‚ç‚¹ï¼Œå®é™…{len(nodes)}"
    assert any("ç¬¬ä¸€ç« " in h for h in headers), "åº”è¯¥è¯†åˆ«ç¬¬ä¸€ç« "
    assert any("ç¬¬äºŒç« " in h for h in headers), "åº”è¯¥è¯†åˆ«ç¬¬äºŒç« "

    print("âœ“ å¤æ‚æ–‡æ¡£è§£ææµ‹è¯•é€šè¿‡")

def test_code_block_preservation():
    """æµ‹è¯•ä»£ç å—ç¼©è¿›å’Œç»“æ„å®Œæ•´æ€§ä¿æŠ¤"""
    print("=== æµ‹è¯•ä»£ç å—ä¿æŠ¤ ===")

    text_content = """ç¬¬ä¸€ç«  ä»£ç ç¤ºä¾‹

Pythonä»£ç ï¼š

```
def complex_function():
    if condition:
        for i in range(10):
            print(f"Item {i}")
        return result
    else:
        return None
```

JavaScriptä»£ç ï¼š

```javascript
function test() {
    const x = 42;
    if (x > 0) {
        console.log("positive");
    }
    return x;
}
```

ç¬¬äºŒç«  ç»“è®º

ç»“è®ºå†…å®¹ã€‚

"""

    blob = Blob(data=text_content.encode("utf-8"), source="code_preserve.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    all_content = "\n".join([node.text for node in nodes])

    # éªŒè¯ä»£ç å—ç¼©è¿›è¢«ä¿ç•™
    assert "    if condition:" in all_content, "Pythonä»£ç ç¼©è¿›åº”è¯¥è¢«ä¿ç•™"
    assert "        for i in range(10):" in all_content, "åµŒå¥—ç¼©è¿›åº”è¯¥è¢«ä¿ç•™"
    assert "    const x = 42;" in all_content, "JavaScriptç¼©è¿›åº”è¯¥è¢«ä¿ç•™"
    assert "        console.log(" in all_content, "JavaScriptåµŒå¥—ç¼©è¿›åº”è¯¥è¢«ä¿ç•™"

    # éªŒè¯ä»£ç å—æ²¡æœ‰è¢«é”™è¯¯åˆå¹¶
    assert "return result\n    else:" in all_content, "ä»£ç å—ç»“æ„åº”è¯¥ä¿æŒå®Œæ•´"

    print("âœ“ ä»£ç å—ä¿æŠ¤æµ‹è¯•é€šè¿‡")

def test_table_preservation():
    """æµ‹è¯•è¡¨æ ¼ç»“æ„ä¿æŠ¤"""
    print("=== æµ‹è¯•è¡¨æ ¼ä¿æŠ¤ ===")

    text_content = """ç¬¬ä¸€ç«  æ•°æ®è¡¨æ ¼

ç”¨æˆ·ä¿¡æ¯è¡¨ï¼š

| ç”¨æˆ·å | å¹´é¾„ | èŒä½ | éƒ¨é—¨ |
|--------|------|------|------|
| alice  | 28   | å·¥ç¨‹å¸ˆ | ç ”å‘éƒ¨ |
| bob    | 32   | è®¾è®¡å¸ˆ | è®¾è®¡éƒ¨ |
| charlie| 25   | æµ‹è¯•å‘˜ | QAéƒ¨  |

ç¬¬äºŒç«  ç»“è®º

ç»“è®ºå†…å®¹ã€‚

"""

    blob = Blob(data=text_content.encode("utf-8"), source="table_preserve.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    all_content = "\n".join([node.text for node in nodes])

    # éªŒè¯è¡¨æ ¼ç»“æ„è¢«ä¿ç•™
    assert "| ç”¨æˆ·å | å¹´é¾„ | èŒä½ | éƒ¨é—¨ |" in all_content, "è¡¨æ ¼æ ‡é¢˜è¡Œåº”è¯¥è¢«ä¿ç•™"
    assert "|--------|------|------|------|" in all_content, "è¡¨æ ¼åˆ†éš”è¡Œåº”è¯¥è¢«ä¿ç•™"
    assert "| alice  | 28   | å·¥ç¨‹å¸ˆ | ç ”å‘éƒ¨ |" in all_content, "è¡¨æ ¼æ•°æ®è¡Œåº”è¯¥è¢«ä¿ç•™"

    print("âœ“ è¡¨æ ¼ä¿æŠ¤æµ‹è¯•é€šè¿‡")

def test_mixed_content_protection():
    """æµ‹è¯•æ··åˆå†…å®¹ï¼ˆä»£ç +è¡¨æ ¼+æ–‡æœ¬+å…¬å¼ï¼‰çš„ä¿æŠ¤"""
    print("=== æµ‹è¯•æ··åˆå†…å®¹ä¿æŠ¤ ===")

    text_content = r"""ç¬¬ä¸€ç«  ç»¼åˆç¤ºä¾‹

è¿™æ˜¯ä¸€ä¸ªç»¼åˆçš„ç¤ºä¾‹ï¼ŒåŒ…å«å¤šç§å†…å®¹ç±»å‹ã€‚

ä»£ç ç¤ºä¾‹ï¼š

```
def calculate_fib(n):
    if n <= 1:
        return n
    else:
        return calculate_fib(n-1) + calculate_fib(n-2)
```

æ•°æ®è¡¨æ ¼ï¼š

| ç®—æ³• | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ |
|------|-----------|-----------|
| æ–æ³¢é‚£å¥‘é€’å½’ | O(2^n)    | O(n)      |
| æ–æ³¢é‚£å¥‘è¿­ä»£ | O(n)      | O(1)      |

æ•°å­¦å…¬å¼ï¼š

$$
F_n = \frac{\phi^n - (-\phi)^{-n}}{\sqrt{5}}
$$

å…¶ä¸­ $\phi = \frac{1 + \sqrt{5}}{2}$ æ˜¯é»„é‡‘æ¯”ä¾‹ã€‚

ç¬¬äºŒç«  åˆ†æ

åˆ†æå†…å®¹ã€‚

"""

    blob = Blob(data=text_content.encode("utf-8"), source="mixed_content.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    all_content = "\n".join([node.text for node in nodes])

    # è°ƒè¯•ï¼šæ‰“å°å®é™…å†…å®¹
    print("å®é™…è§£æå†…å®¹é¢„è§ˆ:")
    print(all_content[:500] + "..." if len(all_content) > 500 else all_content)
    print()

    # éªŒè¯æ‰€æœ‰å†…å®¹ç±»å‹éƒ½è¢«ä¿ç•™
    assert "def calculate_fib(n):" in all_content, "ä»£ç å—åº”è¯¥è¢«ä¿ç•™"
    assert "    if n <= 1:" in all_content, "ä»£ç ç¼©è¿›åº”è¯¥è¢«ä¿ç•™"
    assert "| ç®—æ³• | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ |" in all_content, "è¡¨æ ¼åº”è¯¥è¢«ä¿ç•™"
    assert "æ–æ³¢é‚£å¥‘é€’å½’" in all_content, "è¡¨æ ¼å†…å®¹åº”è¯¥è¢«ä¿ç•™"
    assert r"F_n = \frac{\phi^n - (-\phi)^{-n}}{\sqrt{5}}" in all_content, "æ•°å­¦å…¬å¼åº”è¯¥è¢«ä¿ç•™"
    assert r"\phi = \frac{1 + \sqrt{5}}{2}" in all_content, "å†…è”å…¬å¼åº”è¯¥è¢«ä¿ç•™"

    print("âœ“ æ··åˆå†…å®¹ä¿æŠ¤æµ‹è¯•é€šè¿‡")

def test_large_file_performance():
    """æµ‹è¯•å¤§æ–‡ä»¶å¤„ç†æ€§èƒ½"""
    print("=== æµ‹è¯•å¤§æ–‡ä»¶æ€§èƒ½ ===")

    # ç”Ÿæˆä¸€ä¸ªè¾ƒå¤§çš„æµ‹è¯•æ–‡ä»¶ï¼ˆçº¦1000è¡Œï¼‰
    lines = []
    for i in range(100):
        lines.append(f"ç¬¬{i+1}ç«  ç¬¬{i+1}èŠ‚")
        lines.append(f"è¿™æ˜¯ç¬¬{i+1}ç« çš„å†…å®¹ï¼ŒåŒ…å«äº†ä¸€äº›æè¿°æ€§æ–‡å­—ã€‚")
        lines.append(f"æ›´è¯¦ç»†çš„è¯´æ˜åœ¨è¿™é‡Œï¼ŒåŒ…æ‹¬æŠ€æœ¯ç»†èŠ‚å’Œå®ç°æ–¹æ³•ã€‚")
        lines.append("")  # ç©ºè¡Œåˆ†éš”

        # æ¯10ç« æ·»åŠ ä¸€ä¸ªä»£ç å—
        if (i + 1) % 10 == 0:
            lines.append("ä»£ç ç¤ºä¾‹ï¼š")
            lines.append("```")
            lines.append("def example_function():")
            lines.append("    return 'example'")
            lines.append("```")
            lines.append("")

    large_content = "\n".join(lines)

    blob = Blob(data=large_content.encode("utf-8"), source="large_file.txt")

    start_time = time.time()
    parser = TextParser()
    nodes = parser.parse(blob)
    end_time = time.time()

    processing_time = end_time - start_time
    print(f"å¤„ç†å¤§æ–‡ä»¶è€—æ—¶: {processing_time:.2f}ç§’")
    # æ€§èƒ½æ–­è¨€ï¼šå¤„ç†1000è¡Œåº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
    assert processing_time < 5.0, f"å¤„ç†å¤§æ–‡ä»¶æ—¶é—´è¿‡é•¿: {processing_time:.2f}ç§’"
    assert len(nodes) > 50, f"å¤§æ–‡ä»¶åº”è¯¥ç”Ÿæˆè¾ƒå¤šèŠ‚ç‚¹ï¼Œå®é™…{len(nodes)}"

    print("âœ“ å¤§æ–‡ä»¶æ€§èƒ½æµ‹è¯•é€šè¿‡")

def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("=== æµ‹è¯•è¾¹ç•Œæƒ…å†µ ===")

    # æµ‹è¯•ç©ºæ–‡ä»¶
    blob = Blob(data=b"", source="empty.txt")
    parser = TextParser()
    nodes = parser.parse(blob)
    assert len(nodes) == 1, "ç©ºæ–‡ä»¶åº”è¯¥ç”Ÿæˆä¸€ä¸ªé»˜è®¤èŠ‚ç‚¹"
    assert nodes[0].metadata['section_header'] == "Introduction"

    # æµ‹è¯•åªæœ‰ä»£ç çš„æ–‡ä»¶
    code_only_content = """```
def only_code():
    return True
```
"""
    blob = Blob(data=code_only_content.encode("utf-8"), source="code_only.txt")
    nodes = parser.parse(blob)
    assert len(nodes) == 1
    assert "def only_code():" in nodes[0].text

    # æµ‹è¯•åªæœ‰è¡¨æ ¼çš„æ–‡ä»¶
    table_only_content = """| A | B | C |
|---|---|---|
| 1 | 2 | 3 |
"""
    blob = Blob(data=table_only_content.encode("utf-8"), source="table_only.txt")
    nodes = parser.parse(blob)
    assert len(nodes) == 1
    assert "| A | B | C |" in nodes[0].text

    print("âœ“ è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")

def test_config_options():
    """æµ‹è¯•é…ç½®é€‰é¡¹"""
    print("=== æµ‹è¯•é…ç½®é€‰é¡¹ ===")

    text_content = """ç¬¬ä¸€ç«  å¼•è¨€

è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•
é…ç½®é€‰é¡¹çš„
æ–‡æ¡£ã€‚

ç¬¬äºŒç«  èƒŒæ™¯

èƒŒæ™¯å†…å®¹ã€‚

"""

    # æµ‹è¯•ç¦ç”¨çŸ­è¡Œåˆå¹¶
    config = {'merge_short_lines': False}
    blob = Blob(data=text_content.encode("utf-8"), source="config_test.txt")
    parser = TextParser(config=config)
    nodes = parser.parse(blob)

    all_content = "\n".join([node.text for node in nodes])

    # å½“ç¦ç”¨åˆå¹¶æ—¶ï¼ŒçŸ­è¡Œåº”è¯¥ä¿æŒç‹¬ç«‹
    assert "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\né…ç½®é€‰é¡¹çš„\næ–‡æ¡£ã€‚" in all_content, "ç¦ç”¨åˆå¹¶æ—¶çŸ­è¡Œåº”è¯¥ä¿æŒç‹¬ç«‹"

    # æµ‹è¯•è‡ªå®šä¹‰åˆå¹¶é˜ˆå€¼
    config = {'merge_short_line_threshold': 10}  # å¾ˆå°çš„é˜ˆå€¼
    parser = TextParser(config=config)
    nodes = parser.parse(blob)

    # åº”è¯¥æœ‰æ›´å¤šåˆå¹¶ï¼Œå› ä¸ºé˜ˆå€¼å¾ˆå°
    all_content = "\n".join([node.text for node in nodes])
    # è¿™é‡Œä¸åšä¸¥æ ¼æ–­è¨€ï¼Œå› ä¸ºåˆå¹¶é€»è¾‘å¤æ‚

    print("âœ“ é…ç½®é€‰é¡¹æµ‹è¯•é€šè¿‡")

def test_unicode_and_special_chars():
    """æµ‹è¯•Unicodeå­—ç¬¦å’Œç‰¹æ®Šç¬¦å·"""
    print("=== æµ‹è¯•Unicodeå’Œç‰¹æ®Šå­—ç¬¦ ===")

    text_content = """ç¬¬ä¸€ç«  ç‰¹æ®Šå­—ç¬¦

åŒ…å«å„ç§ç‰¹æ®Šå­—ç¬¦ï¼šÂ©Â®â„¢â‚¬Â£Â¥Â§Â¶â€ â€¡â€¢Â°Â±Ã—Ã·â‰ˆâ‰ â‰¤â‰¥

Unicodeç¬¦å·ï¼šÎ±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰

è¡¨æƒ…ç¬¦å·ï¼šğŸ˜€ğŸ˜‚ğŸ¤”ğŸ‘â¤ï¸ğŸ”¥

ä¸­æ–‡ï¼šä½ å¥½ä¸–ç•Œ
æ—¥æ–‡ï¼šã“ã‚“ã«ã¡ã¯ä¸–ç•Œ
éŸ©æ–‡ï¼šì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„

ç¬¬äºŒç«  ç»“è®º

ç»“è®ºå†…å®¹ã€‚

"""

    blob = Blob(data=text_content.encode("utf-8"), source="unicode_test.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    all_content = "\n".join([node.text for node in nodes])

    # éªŒè¯ç‰¹æ®Šå­—ç¬¦è¢«ä¿ç•™
    assert "Â©Â®â„¢" in all_content, "ç‰¹æ®Šç¬¦å·åº”è¯¥è¢«ä¿ç•™"
    assert "Î±Î²Î³" in all_content, "å¸Œè…Šå­—æ¯åº”è¯¥è¢«ä¿ç•™"
    assert "ğŸ˜€ğŸ˜‚" in all_content, "è¡¨æƒ…ç¬¦å·åº”è¯¥è¢«ä¿ç•™"
    assert "ä½ å¥½ä¸–ç•Œ" in all_content, "ä¸­æ–‡åº”è¯¥è¢«ä¿ç•™"
    assert "ã“ã‚“ã«ã¡ã¯" in all_content, "æ—¥æ–‡åº”è¯¥è¢«ä¿ç•™"
    assert "ì•ˆë…•í•˜ì„¸ìš”" in all_content, "éŸ©æ–‡åº”è¯¥è¢«ä¿ç•™"

    print("âœ“ Unicodeå’Œç‰¹æ®Šå­—ç¬¦æµ‹è¯•é€šè¿‡")

def test_complex_text_parsing(save_results=True):
    """æµ‹è¯•å¤æ‚çš„æ–‡æœ¬è§£æï¼ŒåŒ…æ‹¬å¤šç§å…ƒç´ å’ŒåµŒå¥—ç»“æ„"""
    print("=== æµ‹è¯•å¤æ‚æ–‡æœ¬è§£æ ===")
    
    # æ„é€ å¤æ‚çš„æ–‡æœ¬å†…å®¹
    text_content = r"""ç¬¬ä¸€ç«  å¼•è¨€

è¿™æ˜¯æ–‡æ¡£çš„å¼•è¨€éƒ¨åˆ†ï¼ŒåŒ…å«äº†åŸºæœ¬çš„ä»‹ç»å†…å®¹ã€‚

## ç¬¬äºŒç«  èƒŒæ™¯ä»‹ç»

### å†å²å‘å±•

Markdownäº2004å¹´ç”±John Gruberåˆ›å»ºï¼Œæ—¨åœ¨æä¾›ä¸€ç§æ˜“è¯»æ˜“å†™çš„çº¯æ–‡æœ¬æ ¼å¼ã€‚

#### ç‰ˆæœ¬æ¼”è¿›

- **Markdown 1.0**: æœ€åˆç‰ˆæœ¬
- **CommonMark**: æ ‡å‡†åŒ–è§„èŒƒ
- **GitHub Flavored Markdown**: æ‰©å±•ç‰ˆæœ¬

### ä¸»è¦ç‰¹æ€§

Markdownæ”¯æŒï¼š

1. **æ ‡é¢˜å±‚çº§** (H1-H6)
2. **æ–‡æœ¬æ ¼å¼åŒ–**ï¼š
   - *æ–œä½“*
   - **ç²—ä½“**
   - `ä»£ç `
   - ~~åˆ é™¤çº¿~~
3. **åˆ—è¡¨**ï¼š
   - æ— åºåˆ—è¡¨
   - æœ‰åºåˆ—è¡¨
   - åµŒå¥—åˆ—è¡¨

## ç¬¬ä¸‰ç«  ä»£ç ç¤ºä¾‹

### Pythonä»£ç å—

```
# è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„Pythonå‡½æ•°
def complex_function(data: List[Dict[str, Any]], threshold: float = 0.5) -> Dict[str, Any]:
    \"\"\"å¤æ‚çš„å‡½æ•°å¤„ç†æ•°æ®\"\"\"
    # è¿‡æ»¤æ•°æ®
    filtered = [item for item in data if item.get('score', 0) > threshold]

    # ç»Ÿè®¡åˆ†æ
    stats = {
        'total': len(filtered),
        'average_score': sum(item['score'] for item in filtered) / len(filtered) if filtered else 0,
        'categories': {}
    }

    # åˆ†ç±»ç»Ÿè®¡
    for item in filtered:
        cat = item.get('category', 'unknown')
        stats['categories'][cat] = stats['categories'].get(cat, 0) + 1

    # ## è¿™ä¸æ˜¯æ ‡é¢˜ï¼Œåªæ˜¯åœ¨ä»£ç æ³¨é‡Šä¸­
    return stats

# è°ƒç”¨å‡½æ•°
result = complex_function(sample_data)
print(f"å¤„ç†ç»“æœ: {result}")
```

### JavaScriptä»£ç å—

```javascript
// å¤æ‚çš„JavaScriptå‡½æ•°
class DataProcessor {
    constructor(config) {
        this.config = config;
        // ## è¿™ä¹Ÿä¸æ˜¯æ ‡é¢˜
    }

    async process(data) {
        try {
            // æ•°æ®éªŒè¯
            if (!Array.isArray(data)) {
                throw new Error('æ•°æ®å¿…é¡»æ˜¯æ•°ç»„');
            }

            // å¹¶è¡Œå¤„ç†
            const promises = data.map(async (item) => {
                const processed = await this.transform(item);
                return this.validate(processed);
            });

            return await Promise.all(promises);
        } catch (error) {
            console.error('å¤„ç†å¤±è´¥:', error);
            throw error;
        }
    }

    // ## ç§æœ‰æ–¹æ³•
    transform(item) {
        return {
            ...item,
            processed_at: new Date().toISOString(),
            hash: this.generateHash(item)
        };
    }
}
```

## ç¬¬å››ç«  æ•°å­¦å…¬å¼

### åŸºæœ¬å…¬å¼

å†…è”å…¬å¼ï¼š$E = mc^2$ æ˜¯çˆ±å› æ–¯å¦çš„è´¨èƒ½æ–¹ç¨‹ã€‚

### å¤æ‚å…¬å¼å—

$$
\frac{d}{dx} \int_a^x f(t) \, dt = f(x)
$$

$$
\lim_{x \to 0} \frac{\sin x}{x} = 1
$$

### çŸ©é˜µå’Œæ–¹ç¨‹ç»„

$$
\begin{pmatrix}
a & b \\
c & d
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
=
\begin{pmatrix}
ax + by \\
cx + dy
\end{pmatrix}
$$

### å¤šè¡Œæ–¹ç¨‹

$$
\begin{align}
\nabla \cdot \mathbf{E} &= \frac{\rho}{\epsilon_0} \\
\nabla \cdot \mathbf{B} &= 0 \\
\nabla \times \mathbf{E} &= -\frac{\partial \mathbf{B}}{\partial t} \\
\nabla \times \mathbf{B} &= \mu_0 \mathbf{J} + \mu_0 \epsilon_0 \frac{\partial \mathbf{E}}{\partial t}
\end{align}
$$

## ç¬¬äº”ç«  æ•°æ®è¡¨æ ¼

### åŸºæœ¬è¡¨æ ¼

| å§“å | å¹´é¾„ | èŒä¸š | è–ªèµ„ |
|------|------|------|------|
| å¼ ä¸‰ | 28 | å·¥ç¨‹å¸ˆ | Â¥12000 |
| æå›› | 32 | è®¾è®¡å¸ˆ | Â¥10000 |
| ç‹äº” | 25 | äº§å“ç»ç† | Â¥15000 |

### å¤æ‚è¡¨æ ¼ï¼ˆåŒ…å«Markdownè¯­æ³•ï¼‰

| åŠŸèƒ½ | æè¿° | ç¤ºä¾‹ | çŠ¶æ€ |
|------|------|------|------|
| **æ ‡é¢˜** | æ”¯æŒå¤šçº§æ ‡é¢˜ | # H1<br>## H2<br>### H3 | âœ… |
| *æ ¼å¼åŒ–* | æ–‡æœ¬æ ·å¼ | **ç²—ä½“**<br>*æ–œä½“*<br>`ä»£ç ` | âœ… |
| é“¾æ¥ | å¤–éƒ¨é“¾æ¥ | [Google](https://google.com)<br>[å†…éƒ¨](#section) | âœ… |
| å›¾ç‰‡ | å›¾ç‰‡æ˜¾ç¤º | ![Logo](https://example.com/logo.png) | âœ… |
| åˆ—è¡¨ | åµŒå¥—åˆ—è¡¨ | - é¡¹ç›®1<br>  - å­é¡¹ç›®<br>- é¡¹ç›®2 | âœ… |
| # æ ‡é¢˜æ ‡è®° | è¡¨æ ¼ä¸­çš„æ ‡é¢˜ | # è¿™ä¸æ˜¯æ ‡é¢˜ | âœ… |

### è·¨è¡Œè¡¨æ ¼

| é¡¹ç›® | è¯´æ˜ | çŠ¶æ€ | å¤‡æ³¨ |
|------|------|------|------|
| æ•°æ®å¤„ç† | å®ç°å¤æ‚çš„æ•°æ®å¤„ç†é€»è¾‘ | å®Œæˆ | æ”¯æŒå¤šæ ¼å¼è¾“å…¥ |
| ç”¨æˆ·ç•Œé¢ | è®¾è®¡ç›´è§‚çš„ç”¨æˆ·ç•Œé¢ | è¿›è¡Œä¸­ | ä½¿ç”¨ç°ä»£UIæ¡†æ¶ |
| APIé›†æˆ | ä¸ç¬¬ä¸‰æ–¹APIé›†æˆ | å¾…å¼€å§‹ | éœ€è¦APIå¯†é’¥ |
| æµ‹è¯•è¦†ç›– | ç¼–å†™å…¨é¢çš„å•å…ƒæµ‹è¯• | å®Œæˆ | è¦†ç›–ç‡95% |

## ç¬¬å…­ç«  ç»“è®º

### æ€»ç»“

æœ¬æ–‡æ¡£æ¼”ç¤ºäº†æ–‡æœ¬è§£æå™¨çš„å„ç§å¤æ‚ç‰¹æ€§ï¼š

1. **å¤šçº§æ ‡é¢˜åµŒå¥—**
2. **å¤šç§ä»£ç å—**
3. **å¤æ‚æ•°å­¦å…¬å¼**
4. **ä¸°å¯Œçš„è¡¨æ ¼æ ¼å¼**
5. **å—ä¿æŠ¤é€»è¾‘**

### æœªæ¥å±•æœ›

æœªæ¥å°†ç»§ç»­æ‰©å±•è§£æå™¨çš„åŠŸèƒ½ï¼Œæ”¯æŒæ›´å¤šç°ä»£æ–‡æ¡£éœ€æ±‚ã€‚

CONCLUSION

æ€»ç»“å†…å®¹ã€‚

I. é™„å½•A

é™„å½•å†…å®¹ã€‚

II. é™„å½•B

æ›´å¤šé™„å½•å†…å®¹ã€‚
""".strip()

    blob = Blob(data=text_content.encode("utf-8"), source="complex_test.txt")
    parser = TextParser()
    nodes = parser.parse(blob)

    print(f"æ€»èŠ‚ç‚¹æ•°: {len(nodes)}")

    # éªŒè¯åŸºæœ¬ç»“æ„
    assert len(nodes) >= 8, f"æœŸæœ›è‡³å°‘8ä¸ªèŠ‚ç‚¹ï¼Œå®é™…{len(nodes)}"

    # æ£€æŸ¥æ ‡é¢˜å±‚çº§
    headers = [node.metadata.get('section_header') for node in nodes]
    print(f"è¯†åˆ«åˆ°çš„æ ‡é¢˜: {headers}")

    # æ£€æŸ¥å†…å®¹åŒ…å«
    all_content = "\n".join([node.text for node in nodes])
    assert "def complex_function" in all_content, "åº”è¯¥åŒ…å«Pythonä»£ç "
    assert "class DataProcessor" in all_content, "åº”è¯¥åŒ…å«JavaScriptä»£ç "
    assert "E = mc^2" in all_content, "åº”è¯¥åŒ…å«æ•°å­¦å…¬å¼"
    assert "|" in all_content, "åº”è¯¥åŒ…å«è¡¨æ ¼"

    print("âœ“ å¤æ‚æ–‡æœ¬è§£ææµ‹è¯•é€šè¿‡")

    # å¯é€‰ä¿å­˜ç»“æœåˆ° txt æ–‡ä»¶
    if save_results:
        output_dir = current_dir / "test_output_files"
        output_dir.mkdir(exist_ok=True)
        output_file = output_dir / "text_parser_results.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("Text è§£ææµ‹è¯•ç»“æœ\n")
            f.write("=" * 50 + "\n")
            f.write(f"æ€»èŠ‚ç‚¹æ•°: {len(nodes)}\n\n")
            for i, node in enumerate(nodes):
                f.write(f"èŠ‚ç‚¹ {i}:\n")
                f.write(f"  æ–‡æœ¬: {node.text[:200]}...\n")
                f.write(f"  å…ƒæ•°æ®: {node.metadata}\n")
                f.write("\n")
        print(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹TextParseråŠŸèƒ½æµ‹è¯•...\n")
    try:
        test_basic_text_parsing()
        test_encoding_detection()
        test_header_detection_edge_cases()
        test_line_merging()
        test_header_recognition()
        test_block_protection()
        test_cleaning_features()
        test_load_data()
        test_complex_document()
        test_code_block_preservation()
        test_table_preservation()
        test_mixed_content_protection()
        test_large_file_performance()
        test_edge_cases()
        test_config_options()
        test_unicode_and_special_chars()
        test_complex_text_parsing()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼TextParseråŠŸèƒ½å®Œæ•´ä¸”å¥å£®ã€‚")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    run_all_tests()