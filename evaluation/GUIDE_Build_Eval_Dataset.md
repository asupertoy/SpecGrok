# LlamaIndex è¯„ä¼°æ•°æ®é›†æ„å»ºæŒ‡å— (Evaluation Dataset Construction Guide)

æœ¬æŒ‡å—æ—¨åœ¨æŒ‡å¯¼å¦‚ä½•åŸºäºç°æœ‰æ–‡æ¡£æ•°æ®ï¼Œæ„å»ºä¸€å¥—æ ‡å‡†åŒ–çš„â€œé»„é‡‘æ•°æ®é›†â€ï¼ˆGolden Datasetï¼‰ã€‚è¯¥æ•°æ®é›†å°†åŒ…å«â€œé—®é¢˜-å‚è€ƒæ–‡æ¡£IDâ€å¯¹ï¼ˆQuestion-Context Pairsï¼‰ï¼Œç”¨äºåç»­çš„æ£€ç´¢è¯„ä¼°ï¼ˆRetrieval Evalï¼‰å’Œç”Ÿæˆè¯„ä¼°ï¼ˆResponse Evalï¼‰ã€‚

æ ¸å¿ƒåŸåˆ™
è¯„ä¼°æ•°æ®é›†æ°¸è¿œä»¥ã€ŒChunk / Nodeã€ä¸ºæœ€å°äº‹å®å•å…ƒ
ä¸ç›´æ¥è¯„ä¼°â€œåŸå§‹æ–‡æ¡£â€ï¼Œè€Œè¯„ä¼° Parser + Chunker + Retriever + Generator çš„æ•´ä½“æ•ˆæœ

---

## ğŸ“‹ ä»»åŠ¡åˆ—è¡¨ (Task List)

### Phase 1: ç¯å¢ƒä¸æ•°æ®å‡†å¤‡
- [ ] **ç¡®å®š LLM**: é€‰æ‹©ä¸€ä¸ªé«˜è´¨é‡çš„æ¨¡å‹ï¼ˆå¦‚ GPT-4 æˆ– DeepSeek-V3ï¼‰ç”¨äºç”Ÿæˆé—®é¢˜ã€‚*æ³¨æ„ï¼šç”Ÿæˆæ•°æ®é›†çš„æ¨¡å‹æœ€å¥½ä¼˜äºæˆ–ç­‰åŒäº RAG è¿è¡Œæ—¶ä½¿ç”¨çš„æ¨¡å‹ã€‚*
- [ ] **åŠ è½½æ–‡æ¡£ (Nodes)**: å‡†å¤‡å¥½ç»è¿‡æ¸…æ´—ã€åˆ‡åˆ†åçš„ `List[BaseNode]`ã€‚
    - *æç¤º*: å¯ä»¥å¤ç”¨æ‚¨é¡¹ç›®ä¸­ `ingestion` ç®¡é“ç”Ÿæˆçš„ Nodesã€‚

### Phase 2: æ•°æ®é›†ç”Ÿæˆ (æ ¸å¿ƒ)
- [ ] **ç”Ÿæˆ QA å¯¹**: ä½¿ç”¨ `generate_question_context_pairs` å‡½æ•°è‡ªåŠ¨åˆæˆé—®é¢˜ã€‚
- [ ] **é…ç½®å‚æ•°**: 
    - `num_questions_per_chunk`: å»ºè®®è®¾ç½®ä¸º 1 æˆ– 2ï¼ˆæ¯ä¸ªåˆ‡ç‰‡ç”Ÿæˆçš„é—®é¢˜æ•°ï¼‰ã€‚
    - `llm`: ä¼ å…¥é…ç½®å¥½çš„ LLM å®ä¾‹ã€‚
- [ ] **è¿‡æ»¤ä¸æ¸…æ´— (å¯é€‰)**: äººå·¥æˆ–é€šè¿‡è„šæœ¬æ£€æŸ¥ç”Ÿæˆçš„é—®é¢˜ï¼Œå‰”é™¤å«ç³Šä¸æ¸…æˆ–è¿‡äºç®€å•çš„é—®é¢˜ã€‚

### Phase 3: æŒä¹…åŒ–ä¸åŠ è½½
- [ ] **ä¿å­˜æ•°æ®é›†**: å°†ç”Ÿæˆçš„å¯¹è±¡ä¿å­˜ä¸º JSON æ–‡ä»¶ï¼ˆå¦‚ `eval_dataset_v1.json`ï¼‰ã€‚
- [ ] **éªŒè¯åŠ è½½**: ç¼–å†™æµ‹è¯•ä»£ç ç¡®ä¿èƒ½é€šè¿‡ `EmbeddingQAFinetuneDataset` ç±»æ­£ç¡®è¯»å–ã€‚

### Phase 4: é›†æˆè¯„ä¼°
- [ ] **å¯¹æ¥æ£€ç´¢è¯„ä¼°**: ä½¿ç”¨ `RetrieverEvaluator` æµ‹è¯• Hit Rate å’Œ MRRã€‚
- [ ] **å¯¹æ¥ç”Ÿæˆè¯„ä¼°**: æå–æ•°æ®é›†ä¸­çš„ Queryï¼Œä½¿ç”¨ `BatchEvalRunner` æµ‹è¯• Faithfulness å’Œ Relevancyã€‚

---

## ğŸ› ï¸ è¯¦ç»†å®æ–½ä»£ç å‚è€ƒ

### 1. ç”Ÿæˆå¹¶ä¿å­˜æ•°æ®é›† (Generate & Save)

æ­¤è„šæœ¬ç”¨äºä» Nodes è‡ªåŠ¨ç”Ÿæˆ QA æ•°æ®é›†ã€‚

```python
import os
import asyncio
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.evaluation import generate_question_context_pairs
from llama_index.core.evaluation import EmbeddingQAFinetuneDataset
from llama_index.llms.openai import OpenAI
# from models.llm import get_llm  # å¦‚æœä½ æœ‰è‡ªå®šä¹‰çš„ LLM è·å–æ–¹å¼

async def build_golden_dataset(nodes, output_path="data/golden_dataset.json"):
    """
    è¾“å…¥: nodes (List[BaseNode])
    è¾“å‡º: ä¿å­˜ JSON æ–‡ä»¶
    """
    # 1. é…ç½®ç”Ÿæˆç”¨ LLM (å»ºè®®ä½¿ç”¨èƒ½åŠ›è¾ƒå¼ºçš„æ¨¡å‹ä»¥ä¿è¯é—®é¢˜è´¨é‡)
    # llm = get_llm() 
    llm = OpenAI(model="gpt-4", temperature=0.0)

    print(f"æ­£åœ¨åŸºäº {len(nodes)} ä¸ªèŠ‚ç‚¹ç”Ÿæˆ QA å¯¹...è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚")

    # 2. ç”Ÿæˆæ ¸å¿ƒé€»è¾‘
    # generate_question_context_pairs ä¼šè¿”å›ä¸€ä¸ª EmbeddingQAFinetuneDataset å¯¹è±¡
    # å®ƒåŒ…å«: queries (é—®é¢˜), relevant_docs (é—®é¢˜å¯¹åº”çš„ node_id), corpus (æ‰€æœ‰èŠ‚ç‚¹çš„æ–‡æœ¬)
    dataset = generate_question_context_pairs(
        nodes,
        llm=llm,
        num_questions_per_chunk=1,  # æ¯ä¸ª chunk ç”Ÿæˆ 1 ä¸ªé—®é¢˜ï¼Œé¿å…é—®é¢˜é‡å¤
    )

    # 3. æŒä¹…åŒ–ä¿å­˜
    # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œç¡®ä¿è¯„ä¼°åŸºå‡†å›ºå®šï¼Œæ–¹ä¾¿åç»­å¯¹æ¯”ä¸åŒ Retriever çš„æ•ˆæœ
    dataset.save_json(output_path)
    print(f"âœ… æ•°æ®é›†å·²ä¿å­˜è‡³: {output_path}")
    
    # æ‰“å°ç¤ºä¾‹
    first_query_id = list(dataset.queries.keys())[0]
    print(f"ç¤ºä¾‹ Question: {dataset.queries[first_query_id]}")
    print(f"å…³è” Node ID: {dataset.relevant_docs[first_query_id]}")

# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    # å‡è®¾ä½ å·²ç»æœ‰äº† nodesï¼Œå¦‚æœæ²¡æœ‰ï¼Œä¸´æ—¶åŠ è½½ï¼š
    # reader = SimpleDirectoryReader("./data/raw")
    # documents = reader.load_data()
    # nodes = ... (æ‰§è¡Œä½ çš„ Chunking é€»è¾‘)
    
    # asyncio.run(build_golden_dataset(nodes))
    pass
```

### 2.åŠ è½½æ•°æ®é›†è¿›è¡Œæ£€ç´¢è¯„ä¼° (Load & Eval Retrieval)
æ­¤è„šæœ¬å±•ç¤ºå¦‚ä½•è¯»å–åˆšæ‰ä¿å­˜çš„ JSONï¼Œå¹¶å¯¹å½“å‰çš„ Retriever è¿›è¡Œæ‰“åˆ†ã€‚

```python
import asyncio
import pandas as pd
from llama_index.core.evaluation import RetrieverEvaluator, EmbeddingQAFinetuneDataset
from llama_index.core import VectorStoreIndex

# å‡è®¾ä½ å·²ç»æ„å»ºå¥½äº† index
# from src.database.client import get_index 

async def run_retrieval_eval(dataset_path="data/golden_dataset.json"):
    # 1. åŠ è½½é»„é‡‘æ•°æ®é›†
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
    dataset = EmbeddingQAFinetuneDataset.from_json(dataset_path)

    # 2. å‡†å¤‡ Retriever (å¾…è¯„ä¼°å¯¹è±¡)
    # index = get_index()
    # retriever = index.as_retriever(similarity_top_k=5)
    retriever = ... # åˆå§‹åŒ–ä½ çš„ retriever

    # 3. å®šä¹‰è¯„ä¼°å™¨
    # hit_rate: æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨ top_k ä¸­
    # mrr: æ­£ç¡®ç­”æ¡ˆæ’åçš„å€’æ•° (Mean Reciprocal Rank)
    retriever_evaluator = RetrieverEvaluator.from_metric_names(
        ["mrr", "hit_rate"], retriever=retriever
    )

    # 4. æ‰¹é‡è¿è¡Œè¯„ä¼° (aevaluate_dataset æ˜¯ä¸“é—¨é’ˆå¯¹ EmbeddingQAFinetuneDataset çš„ä¼˜åŒ–æ–¹æ³•)
    print("å¼€å§‹è¿è¡Œæ‰¹é‡è¯„ä¼°...")
    eval_results = await retriever_evaluator.aevaluate_dataset(dataset)

    # 5. å±•ç¤ºç»“æœ
    metric_dicts = []
    for eval_result in eval_results:
        metric_dicts.append(eval_result.metric_vals_dict)

    df = pd.DataFrame(metric_dicts)
    print("\n------------------ è¯„ä¼°æŠ¥å‘Š ------------------")
    print(f"å¹³å‡ Hit Rate: {df['hit_rate'].mean():.4f}")
    print(f"å¹³å‡ MRR:      {df['mrr'].mean():.4f}")
    print("---------------------------------------------")

if __name__ == "__main__":
    # asyncio.run(run_retrieval_eval())
    pass
```

### 3.ç”¨äºç”Ÿæˆè¯„ä¼° (Response Eval)
è™½ç„¶ generate_question_context_pairs ä¸»è¦ç”¨äºæ£€ç´¢è¯„ä¼°ï¼Œä½†ç”Ÿæˆçš„ Questions åˆ—è¡¨åŒæ ·å¯ä»¥ç›´æ¥ç”¨äºç”Ÿæˆè¯„ä¼°ã€‚

```python
from llama_index.core.evaluation import BatchEvalRunner, FaithfulnessEvaluator, RelevancyEvaluator
from llama_index.core.evaluation import EmbeddingQAFinetuneDataset

async def run_response_eval(dataset_path="data/golden_dataset.json", query_engine=None):
    # 1. åŠ è½½æ•°æ®é›†
    dataset = EmbeddingQAFinetuneDataset.from_json(dataset_path)
    questions = list(dataset.queries.values()) # æå–æ‰€æœ‰é—®é¢˜åˆ—è¡¨

    # 2. å®šä¹‰è¯„ä¼°å™¨ (Faithfulness & Relevancy)
    # è¿™é‡Œçš„ llm å……å½“â€œè£åˆ¤â€
    # evaluator_llm = OpenAI(model="gpt-4")
    # faithfulness = FaithfulnessEvaluator(llm=evaluator_llm)
    # relevancy = RelevancyEvaluator(llm=evaluator_llm)

    # 3. æ‰¹é‡è¿è¡Œ
    # runner = BatchEvalRunner(
    #    {"faithfulness": faithfulness, "relevancy": relevancy},
    #    workers=8
    # )
    
    # 4. æ‰§è¡Œè¯„ä¼°
    # eval_results = await runner.aevaluate_queries(
    #    query_engine, queries=questions
    # )
    pass
```

#### æ³¨æ„äº‹é¡¹
1.æ•°æ®éš”ç¦»: ç¡®ä¿ç”¨äºè¯„ä¼°çš„æ–‡æ¡£å·²ç»åŒ…å«åœ¨ä½ çš„ Vector Store (Index) ä¸­ï¼Œå¦åˆ™ Retrieval Eval çš„ Hit Rate å°†æ°¸è¿œä¸º 0ã€‚
2.æˆæœ¬æ§åˆ¶: generate_question_context_pairs ä¼šå¯¹æ¯ä¸ª chunk è°ƒç”¨ä¸€æ¬¡ LLMã€‚å¦‚æœä½ æœ‰ 4000 ä¸ªæ–‡æ¡£ï¼Œå…¨éƒ¨ç”Ÿæˆå¯èƒ½æˆæœ¬è¾ƒé«˜ã€‚å»ºè®®å…ˆé‡‡æ · 50-100 ä¸ªä»£è¡¨æ€§æ–‡æ¡£æ„å»ºä¸€ä¸ªå°å‹çš„ v0.1 æ•°æ®é›†ã€‚
3.Human-in-the-loop: è‡ªåŠ¨ç”Ÿæˆçš„ QA å¯¹å¯èƒ½å¶å°”åŒ…å«å¹»è§‰æˆ–æŒ‡ä»£ä¸æ˜ï¼ˆä¾‹å¦‚é—®â€œæœ¬æ–‡çš„ä½œè€…æ˜¯è°ï¼Ÿâ€ä½†ä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰åå­—ï¼‰ã€‚å»ºè®®åœ¨ä¿å­˜ json åï¼Œäººå·¥å¿«é€Ÿæµè§ˆä¸€é queriesï¼Œåˆ é™¤ä½è´¨é‡é—®é¢˜ã€‚